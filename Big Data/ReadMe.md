# Big-Data-Analysis
## CLO: 03
### (Time Duration: 4 Weeks)

# 1. Introduction to Big Data:
This part will be covering the topics of Big Data handling by using PySpark, MongoDB and Hadoop. We will learn to implement different learning models and storing data in databases and then deploying these models. 
# 2. Course Learning Objectives

3. To understand the Big Data Engineering	 (5 weeks/ Lecture 9/18)
# 3. List of Software
#### 2. Java setup	
(Open source)
#### 3. Apache Spark 
(Open source)
#### 4. Hadoop	
(Open source)
#### 5. AWS	
(Free tier account)

# 4. Lecture Breakdown:
There will be 2 lectures per week. Lecture wise details of the topics is decribed in th next section. 

# 5. Introduction to the Topics:
### 9.	Introduction to Big Data and PySpark (part 1):
Big Data as the digital trace that we are generating in this digital era. In this course, you will learn about the characteristics of Big Data and its application in Big Data Analytics. You will gain an understanding about the features, benefits, limitations, and applications of some of the Big Data processing tools. And basic into of the PySpark.
### 10.	Introduction to Big Data and PySpark (part 2):
Apache Spark is an open-source processing engine that provides users new ways to store and make use of big data. It is an open-source processing engine built around speed, ease of use, and analytics. In this course, you will discover how to leverage Spark to deliver reliable insights. The course provides an overview of the platform, going into the different components that make up Apache Spark.
### 11.	Database:
This lecture will introduce students to the basics of the Structured Query Language (SQL) as well as basic database design for storing data as part of a multi-step data gathering, analysis, and processing effort.  
### 12.	MongoDB:
This lecture will cover the emerging database technology i.e. NoSQL/MongoDB design for storing data as part of a multi-step data gathering, analysis, and processing effort.  
### 13.	Integration:
This lecture will cover the integration of the Apache Spark with the databases of MogoDB and MySQL to perform ETL operations to make data pipline to manage streaming data like stocks data.


### 14.	Pipeline and ETL (part 1):
After integrating the Databases, we need to perform operations on databases using apache spark. This lecture will include the basic operations and concepts of ETL.
### 15.	Pipeline and ETL (part 2):
This lecture will include the advanced operations of the databases with apache spark and perform high level of ETL methods to deal with streaming data .
### 16.	Pipeline and ETL (part 3):
We use Cloud to make the data processing faster and secure. This lecture will be covering integration of the apache spark with cloud.
### 17.	Hadoop and Hive SQL (Part 1):
Hadoop is an open-source framework that allows for the distributed processing of large data sets across clusters of computers using simple programming models. Hive, a data warehouse software, provides an SQL-like interface to efficiently query and manipulate large data sets residing in various databases and file systems that integrate with Hadoop.
### 18.	Hadoop and Hive SQL (Part 2):
Hadoop concepts of MapReduce and HIVE will be covered in this lectures.  This lecture will train the individuals to deal with implementation of big data concepts and streaming data. 

